{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This script is a practice of applying CNN for sentence polarity classification. The goal for myself is to reproduce [Yoon Kim (2014)](https://arxiv.org/pdf/1408.5882.pdf)'s results on Rotten Tomatoes' movie reviews to get my hands dirty on CNN/TensorFlow. This goal, though, was unexpectedly tough to achieve. The [author's source code](https://github.com/yoonkim/CNN_sentence) is in Keras. [Denny Britz](https://github.com/dennybritz/cnn-text-classification-tf) has a TensorFlow implementation but it is too advanced for beginners. In addition, all the novice-friendly CNN codes are for images, giving me a real hard time translating the image context to text. \n",
    "\n",
    "Therefore, another goal of this script is to provide a beginner-friendly TensorFlow implementation of CNN on text. The script mimicked ** [Hvass-Labs' CNN tutorial](https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/02_Convolutional_Neural_Network.ipynb) (here is the [video tutorial](https://www.youtube.com/watch?v=HMcx-zY8JSg&t=1503s); the whole series is HIGHLY RECOMMEND for TensoFlow/DeepLearning absolute beginners)**. If you unfortunately decide to pick up CNN from modeling text, hopefully this script can be of some help to you.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "TRAINED_WORDVEC = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "WORDVEC_LEN = TRAINED_WORDVEC.syn0.shape[1]   # 300\n",
    "WORDS_PER_DOC = 50\n",
    "\n",
    "DOC_SHAPE = (WORDS_PER_DOC, WORDVEC_LEN) \n",
    "DOC_FLATTEN_LEN = WORDVEC_LEN * WORDS_PER_DOC\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "NUM_CHANNELS = 1\n",
    "FILTER_HEIGHT = [3,4,5]   # Unlike in images, filter on word embedding should always be \n",
    "                          # as wide as the embedding; varying the size of the filter really is \n",
    "                          # just changing the height\n",
    "DORPOUT_KEEP_PROB = 0.5\n",
    "\n",
    "BATCH_SIZE = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "neg = []\n",
    "with open(\"rt/rt-polarity.neg\") as f:\n",
    "    for line in f:\n",
    "        txt = helper.txt_to_words(line.strip(),remove_stopwords=False)\n",
    "        label = [0,1]\n",
    "        length = len(txt)\n",
    "        neg.append([txt,label,length])\n",
    "        \n",
    "pos = []\n",
    "with open(\"rt/rt-polarity.pos\") as f:\n",
    "    for line in f:\n",
    "        txt = helper.txt_to_words(line.strip(),remove_stopwords=False)\n",
    "        label = [1,0]\n",
    "        length = len(txt)\n",
    "        pos.append([txt,label,length])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "neg_array = np.array(neg)\n",
    "pos_array = np.array(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10662, 3)\n",
      "---> X:\n",
      " [['simplistic', 'silly', 'and', 'tedious']\n",
      " ['it', \"'s\", 'so', 'laddish', 'and', 'juvenile', 'only', 'teenage', 'boys', 'could', 'possibly', 'find', 'it', 'funny']\n",
      " ['exploitative', 'and', 'largely', 'devoid', 'of', 'the', 'depth', 'or', 'sophistication', 'that', 'would', 'make', 'watching', 'such', 'a', 'graphic', 'treatment', 'of', 'the', 'crimes', 'bearable']]\n",
      "---> y:\n",
      " [[0, 1] [0, 1] [0, 1]]\n"
     ]
    }
   ],
   "source": [
    "rt = np.concatenate([neg_array, pos_array], axis=0)\n",
    "print(rt.shape)\n",
    "print(\"---> X:\\n\", rt[:3,0])\n",
    "print(\"---> y:\\n\", rt[:3,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max doc length:  53\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEKCAYAAAAb7IIBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fcfc4cf1710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Distrubiton of doc length\n",
    "plt.hist(rt[:,2].astype(int))\n",
    "plt.title(\"Distribution of doc length\")\n",
    "print(\"Max doc length: \",np.max(rt[:,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.shuffle(rt)\n",
    "X_train, X_test, y_train, y_test = train_test_split(rt[:,0],rt[:,1],test_size = 0.1, random_state =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Before:\n",
      "(9595,)\n",
      "[[1, 0] [1, 0] [1, 0] ..., [1, 0] [0, 1] [1, 0]]\n",
      "---> After:\n",
      "(9595, 2)\n",
      "[[1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " ..., \n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]]\n"
     ]
    }
   ],
   "source": [
    "# Fix dimension of y - make it one-hot encoding\n",
    "print(\"---> Before:\")\n",
    "print(y_train.shape)\n",
    "print(y_train)\n",
    "y_train = np.array(y_train.tolist())\n",
    "y_test = np.array(y_test.tolist())\n",
    "\n",
    "print(\"---> After:\")\n",
    "print(y_train.shape)\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure a simple CNN\n",
    "    * 1 convolutional layer (with drop out)\n",
    "    * 1 fully connected layer (with l2 regularization)\n",
    "\n",
    "#### Helper funtions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Helper to get a parameters to optimize\n",
    "def new_weights(shape):\n",
    "    return tf.Variable(tf.truncated_normal(shape, stddev=0.05))\n",
    "\n",
    "def new_biases(length):\n",
    "    return tf.Variable(tf.constant(0.05,shape =[length]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Configure the convolutional layer\n",
    "def new_conv_layer(input_tensor_4d,\n",
    "                  num_channels,\n",
    "                  filter_heights,\n",
    "                  filter_width,\n",
    "                  num_filters,\n",
    "                  use_pooling=True):\n",
    "    # input_tensor_4d should be a 4D tensor\n",
    "    # input_tensor_4d's dimension = [docs_per_batch, row/words_per_doc, col/latentdims_per_word, num_channels]\n",
    "    \n",
    "    # Create a list for each filter size; \n",
    "    # We have 3 filter sizes of 3,4,5 here, so we'll end up with 3 elemets\n",
    "    # each element is a 4D tensor of [1,1,1,num_filters]\n",
    "    conv_per_filter_size = []\n",
    "    \n",
    "    for filter_height in filter_heights:\n",
    "    \n",
    "        shape_filter = [filter_height, filter_width, num_channels, num_filters]\n",
    "\n",
    "        # Params to learn & optimize\n",
    "        weights = new_weights(shape=shape_filter)\n",
    "        biases = new_biases(length=num_filters)\n",
    "\n",
    "        # Construct the conv layer and compute output\n",
    "        conv_1filter_4d = tf.nn.conv2d(input=input_tensor_4d,\n",
    "                                     filter=weights,\n",
    "                                     strides=[1,1,1,1],  # strides[1,2]: step size rowwise and colwise\n",
    "                                     padding='VALID')    # VALID: no padding \n",
    "        # Add bias\n",
    "        conv_1filter_4d = tf.nn.bias_add(conv_1filter_4d,biases)\n",
    "\n",
    "        # Max-over pooling: take the largest value per filter\n",
    "        conv_1filter_4d = tf.nn.max_pool(value=conv_1filter_4d,\n",
    "                                       ksize=[1,input_tensor_4d.get_shape()[1]- filter_height + 1,1,1],  \n",
    "                                       # window size over which to calculate the max value\n",
    "                                       strides=[1,1,1,1],\n",
    "                                       padding=\"VALID\")\n",
    "\n",
    "        # ReLU - change negative values in the conv output to 0\n",
    "        conv_1filter_4d = tf.nn.relu(conv_1filter_4d)\n",
    "        \n",
    "        conv_per_filter_size.append(conv_1filter_4d)\n",
    "    \n",
    "    # Concatenate the result for each filter size\n",
    "    conv_layer_4d = tf.concat(conv_per_filter_size,axis=3) \n",
    "    \n",
    "    return conv_layer_4d,weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# flattern the conv layer output from 4D to 2D to feed to the final fully-connected layer\n",
    "def flatten_layer(conv_layer_tensor_4d):\n",
    "    \n",
    "    layer_shape = conv_layer_tensor_4d.get_shape()\n",
    "    \n",
    "    num_features = layer_shape[1:4].num_elements()\n",
    "    \n",
    "    layer_2d = tf.reshape(conv_layer_tensor_4d, [-1,num_features])\n",
    "    # layer_2d is [docs_per_batch, row/words_per_doc * col/latentdims_per_word * num_channels]\n",
    "    # in the text example, row/words_per_doc = col/latentdims_per_word = 1\n",
    "    \n",
    "    return layer_2d, num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add a drop out\n",
    "def dropout_layer(input_tensor_2d):\n",
    "    return tf.nn.dropout(input_tensor_2d, keep_prob = DROPOUT_KEEP_PROB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def new_fc_layer(input_tensor_2d,\n",
    "                num_inputs,\n",
    "                num_outputs):\n",
    "    weights = new_weights(shape=[num_inputs,num_outputs])\n",
    "    biases = new_biases(length=num_outputs)\n",
    "    \n",
    "    layer_2d = tf.matmul(input_tensor_2d,weights) + biases\n",
    "    \n",
    "    return layer_2d,weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Build the \"Framework\"\n",
    "##### 1. Input - training X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "x_doc_vec = tf.placeholder(tf.float32, shape=[None, DOC_FLATTEN_LEN])\n",
    "x_doc_vec_to_4d_tensor = tf.reshape(x_doc_vec, shape=[-1,WORDS_PER_DOC,WORDVEC_LEN,1])\n",
    "\n",
    "y_true = tf.placeholder(tf.float32, shape = [None,2]) # One-hot encoded\n",
    "y_true_cls = tf.argmax(y_true, dimension=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##### 2. Build the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'concat_2:0' shape=(?, 1, 1, 300) dtype=float32>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The first layer: convolutional layer with pooling\n",
    "# We have 3 filter size, using 100 filters for each will give us 300 filters\n",
    "layer1_conv1, weights_layer1 = new_conv_layer(input_tensor_4d=x_doc_vec_to_4d_tensor,\n",
    "                                          num_channels=1,\n",
    "                                          filter_heights=FILTER_HEIGHT,\n",
    "                                          filter_width=WORDVEC_LEN,\n",
    "                                          num_filters=100,\n",
    "                                          use_pooling=True)\n",
    "\n",
    "layer1_conv1 # 4D (It's in fact 2D, since it reduces to 1 number per doc per filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Reshape_5:0' shape=(?, 300) dtype=float32>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Flatten the convolutional layer\n",
    "layer1_conv1_2d, num_features= flatten_layer(layer1_conv1)\n",
    "layer1_conv1_2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Implement a dropout on the flattened convolutional layer\n",
    "# After dropout, the shape of the tensor remains the same \n",
    "# - only 50% of values are \"turned off\" (set to 0) with DROPOUT_KEEP_PROB = 0.5\n",
    "layer1_conv1_2d_dropout = dropout_layer(layer1_conv1_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'add_1:0' shape=(?, 2) dtype=float32>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The second layer: fully-connected layer\n",
    "layer2_fc1, weights_layer2 = new_fc_layer(input_tensor_2d=layer1_conv1_2d_dropout,\n",
    "                                         num_inputs=num_features,\n",
    "                                         num_outputs=2)\n",
    "\n",
    "layer2_fc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Softmax_1:0' shape=(?, 2) dtype=float32>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prediction\n",
    "y_pred = tf.nn.softmax(layer2_fc1)\n",
    "y_pred_cls = tf.argmax(y_pred,dimension=1)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Cost function to be optimized\n",
    "# TF built-in function calculates the softmax internally \n",
    "# so we must use the output of layer_fc2 directly rather than y_pred which has already had the softmax applied.\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=layer2_fc1,\n",
    "                                                        labels=y_true)\n",
    "loss = tf.reduce_mean(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=1e-3).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Performance Measures\n",
    "correct_prediction = tf.equal(y_true_cls,y_pred_cls)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Feed in the data and optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batches = helper.generate_batch_indices(batch_size=BATCH_SIZE,\n",
    "                                        data_size=len(X_train))\n",
    "total_iterations = 0\n",
    "#train_array[batches[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def optimize(num_iterations):\n",
    "    global total_iterations\n",
    "    global batches\n",
    "    \n",
    "    t0 = datetime.now()\n",
    "    \n",
    "    for i in range(total_iterations, total_iterations + num_iterations):\n",
    "        \n",
    "        if i >= len(batches):\n",
    "            # Shuffle and generate new batches\n",
    "            batches = helper.generate_batch_indices(batch_size=BATCH_SIZE,\n",
    "                                                    data_size=X_train.shape[0])\n",
    "            multiplier = i // len(batches)\n",
    "            \n",
    "            batch = batches[i - multiplier*len(batches)]\n",
    "            \n",
    "        else:\n",
    "            batch = batches[i]\n",
    "            \n",
    "        y_true_onehot_batch = y_train[batch]\n",
    "        x_batch = helper.create_X_rt(X_train[batch],\n",
    "                                     trained_wordvec=TRAINED_WORDVEC,\n",
    "                                     words_per_doc = WORDS_PER_DOC,\n",
    "                                     wordvec_len = WORDVEC_LEN)\n",
    "        \n",
    "        feed_dict_train = {\n",
    "            x_doc_vec: x_batch,\n",
    "            y_true: y_true_onehot_batch\n",
    "        }\n",
    "        \n",
    "        session.run(optimizer, feed_dict = feed_dict_train)\n",
    "        \n",
    "        if i % 90 == 0:\n",
    "            # Calculate the accuracy on the training-set.\n",
    "            acc = session.run(accuracy, feed_dict=feed_dict_train)\n",
    "\n",
    "            # Message for printing.\n",
    "            msg = \"Optimization Iteration: {0:>6}, Training Accuracy: {1:>6.1%}\"\n",
    "\n",
    "            # Print it.\n",
    "            print(msg.format(i + 1, acc))\n",
    "        \n",
    "    total_iterations += num_iterations\n",
    "    \n",
    "    t1 = datetime.now() - t0\n",
    "    print(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "session = tf.Session() # start a session\n",
    "session.run(tf.global_variables_initializer()) # initialize all the variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batches_test = helper.generate_batch_indices(batch_size=BATCH_SIZE,data_size=X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def predict_on_test():\n",
    "    \n",
    "    global batches_test\n",
    "    \n",
    "    t0 = datetime.now()\n",
    "    \n",
    "    y_true_cls = np.argmax(y_test, axis=1)  # Row-wise\n",
    "    y_pred = np.zeros(y_test.shape[0])\n",
    "    \n",
    "    for i in range(len(batches_test)):\n",
    "        \n",
    "        x_batch = helper.create_X_rt(X_test[batches_test[i]],\n",
    "                                     trained_wordvec=TRAINED_WORDVEC,\n",
    "                                     words_per_doc = WORDS_PER_DOC,\n",
    "                                     wordvec_len = WORDVEC_LEN)\n",
    "        feed_dict_test = {\n",
    "            x_doc_vec: x_batch\n",
    "        }\n",
    "        \n",
    "        y_pred[i*BATCH_SIZE:(i+1)*BATCH_SIZE] = session.run(y_pred_cls,\n",
    "                                                            feed_dict=feed_dict_test)\n",
    "     \n",
    "    correct = (y_true_cls==y_pred)\n",
    "    \n",
    "    accuracy = float(sum(correct)) / X_test.shape[0]\n",
    "    \n",
    "    print(\"# of iterations: \", total_iterations)\n",
    "     # Print the accuracy.\n",
    "    msg = \"Accuracy on Test-Set: {0:.1%} ({1} / {2})\"\n",
    "    print(msg.format(accuracy, sum(correct), X_test.shape[0]))\n",
    "    print(datetime.now() - t0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of iterations:  0\n",
      "Accuracy on Test-Set: 49.6% (529 / 1067)\n",
      "0:00:02.989496\n"
     ]
    }
   ],
   "source": [
    "predict_on_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization Iteration:      1, Training Accuracy:  77.0%\n",
      "0:01:16.675955\n",
      "# of iterations:  90\n",
      "Accuracy on Test-Set: 48.6% (519 / 1067)\n",
      "0:00:02.880818\n"
     ]
    }
   ],
   "source": [
    "optimize(num_iterations=90)  # 90 * 100 = 9000, ~1 epoch\n",
    "predict_on_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization Iteration:     91, Training Accuracy:  68.0%\n",
      "Optimization Iteration:    181, Training Accuracy:  89.0%\n",
      "Optimization Iteration:    271, Training Accuracy:  91.0%\n",
      "Optimization Iteration:    361, Training Accuracy:  93.0%\n",
      "Optimization Iteration:    451, Training Accuracy:  98.0%\n",
      "Optimization Iteration:    541, Training Accuracy: 100.0%\n",
      "Optimization Iteration:    631, Training Accuracy: 100.0%\n",
      "Optimization Iteration:    721, Training Accuracy: 100.0%\n",
      "Optimization Iteration:    811, Training Accuracy: 100.0%\n",
      "Optimization Iteration:    901, Training Accuracy: 100.0%\n",
      "0:12:13.554065\n",
      "# of iterations:  990\n",
      "Accuracy on Test-Set: 47.0% (501 / 1067)\n",
      "0:00:02.793208\n",
      "\n",
      "Optimization Iteration:    991, Training Accuracy: 100.0%\n",
      "Optimization Iteration:   1081, Training Accuracy: 100.0%\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "\n",
    "while counter < 10:\n",
    "    optimize(900)\n",
    "    predict_on_test()\n",
    "    print()\n",
    "    \n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
